[{"title":"明日方舟Wiki数据爬取","url":"/2024/07/25/%E6%98%8E%E6%97%A5%E6%96%B9%E8%88%9FWiki%E6%95%B0%E6%8D%AE%E7%88%AC%E5%8F%96/","content":"前言Wiki百科是动态网页，我们爬取的方法是从网页获取url再在谷歌重新打开。详细参考python爬虫实战 scrapy+selenium爬取动态网页代码源文件以及成果展示arknights_wiki或者\ngh repo clone muqiuwu/arknights_wiki\n数据爬取蜘蛛头文件import scrapyfrom douban.items import DoubanItemfrom bs4 import BeautifulSoupimport pandas as pdimport matplotlib.pyplot as plt###头文件plt.rcParams[&#x27;font.sans-serif&#x27;] = &#x27;SimSun&#x27;plt.rcParams[&#x27;axes.unicode_minus&#x27;] = Falseganyuan = pd.read_excel(&#x27;干员总览.xlsx&#x27;, sheet_name=&#x27;Sheet1&#x27;)###设置中文格式\n\n干员爬取如果我们要爬取干员，有两个方法，第一种是直接在维基干员首页爬取干员信息。但这样有个问题，就是只能爬取前五十个，我的解决方法是爬取先锋干员,以此类推。\nallowed_domains = [&#x27;prts.wiki/&#x27;]start_urls = [&#x27;https://prts.wiki/w/%E5%B9%B2%E5%91%98%E4%B8%80%E8%A7%88#&#x27;]# allowed_domains = [&#x27;space.bilibili.com&#x27;] # start_urls = [&#x27;https://space.bilibili.com/1629347259/video&#x27;] # page = 10 # 如果想要爬取多页数据，需要定义爬取初始的页数，这里是10# 后续页数的网址，需要自己观察翻页后网址的变化得出规律# base_url = &#x27;https://space.bilibili.com/1629347259/video?tid=0&amp;page=&#123;&#125;&#x27;\n这里wiki里每一格是一个干员，我们可以写一个循环，方便爬取\n# index    //*[@id=&quot;filter-result&quot;]/div[1]# name     //*[@id=&quot;filter-result&quot;]/div[1]/div[2]/div/div[1]# cname    //*[@id=&quot;filter-result&quot;]/div[1]/div[2]/div/a/div# cname    //*[@id=&quot;filter-result&quot;]/div[1]/div[2]/div/a/div# hp       //*[@id=&quot;filter-result&quot;]/div[1]/div[4]/div[1]# atk      //*[@id=&quot;filter-result&quot;]/div[1]/div[4]/div[2]# re_ploy  //*[@id=&quot;filter-result&quot;]/div[1]/div[5]/div[1]# code     //*[@id=&quot;filter-result&quot;]/div[1]/div[2]/div/div[3]# 检查网页，得到这些信息的路径    def parse(self, response):        ###### 下面是自己对html数据的处理逻辑                movie_list = response.xpath(&quot;//*[@id=\\&quot;filter-result\\&quot;]/div&quot;)        if movie_list and len(movie_list) &gt; 0:            for movie in movie_list:                item = DoubanItem()                item[&#x27;cname&#x27;] = movie.xpath(&quot;normalize-space(./div[2]/div/a/div/text())&quot;).get()                item[&#x27;ename&#x27;] = movie.xpath(&quot;normalize-space(./div[2]/div/div[1]/text())&quot;).extract()[0]                item[&#x27;jname&#x27;] = movie.xpath(&quot;normalize-space(./div[2]/div/div[2]/text())&quot;).extract()[0]                item[&#x27;code&#x27;] = movie.xpath(&quot;normalize-space(./div[2]/div/div[3]/text())&quot;).extract()[0]                item[&#x27;sub_occupation&#x27;] = movie.xpath(&quot;normalize-space(./div[3]/div/div[1]/text())&quot;).extract()[0]                item[&#x27;influnce&#x27;] = movie.xpath(&quot;normalize-space(./div[3]/div/div[2]/text())&quot;).extract()[0]                item[&#x27;place_of_birth&#x27;] = movie.xpath(&quot;normalize-space(./div[3]/div/div[3]/text())&quot;).extract()[0]                item[&#x27;race&#x27;] = movie.xpath(&quot;normalize-space(./div[3]/div/div[4]/text())&quot;).extract()[0]                item[&#x27;hp&#x27;] = movie.xpath(&quot;normalize-space(./div[4]/div[1]/text())&quot;).get()                item[&#x27;atk&#x27;] = movie.xpath(&quot;normalize-space(./div[4]/div[2]/text())&quot;).extract()[0]                item[&#x27;defe&#x27;] = movie.xpath(&quot;normalize-space(./div[4]/div[3]/text())&quot;).extract()[0]                item[&#x27;res&#x27;] = movie.xpath(&quot;normalize-space(./div[4]/div[4]/text())&quot;).extract()[0]                item[&#x27;re_deploy&#x27;] = movie.xpath(&quot;normalize-space(./div[5]/div[1]/text())&quot;).extract()[0]                item[&#x27;cost&#x27;] = movie.xpath(&quot;normalize-space(./div[5]/div[2]/text())&quot;).extract()[0]                item[&#x27;block&#x27;] = movie.xpath(&quot;normalize-space(./div[5]/div[3]/text())&quot;).extract()[0]                item[&#x27;interval&#x27;] = movie.xpath(&quot;normalize-space(./div[5]/div[4]/text())&quot;).extract()[0]                item[&#x27;sex&#x27;] = movie.xpath(&quot;normalize-space(./div[6]/div[1]/text())&quot;).extract()[0]                item[&#x27;position&#x27;] = movie.xpath(&quot;normalize-space(./div[6]/div[2]/text())&quot;).extract()[0]                item[&#x27;obtain&#x27;] = [movie.xpath(f&quot;normalize-space(./div[7]/div[&#123;i&#125;]/text())&quot;).get()                                  for i in range(1,5)]                item[&#x27;tag&#x27;] = movie.xpath(&quot;normalize-space(./div[8]/div[1]/text())&quot;).get(),movie.xpath(&quot;normalize-space(./div[8]/div[2]/text())&quot;).extract()[0],movie.xpath(&quot;normalize-space(./div[8]/div[3]/text())&quot;).extract()[0],movie.xpath(&quot;normalize-space(./div[8]/div[4]/text())&quot;).extract()[0]                item[&#x27;feature&#x27;] = movie.xpath(&quot;normalize-space(./div[9]/div/div)&quot;).get()                yield item\n藏品爬取和干员爬取相似。\nallowed_domains = [&#x27;prts.wiki/&#x27;]    start_urls = [&#x27;https://prts.wiki/w/%E8%90%A8%E5%8D%A1%E5%85%B9%E7%9A%84%E6%97%A0%E7%BB%88%E5%A5%87%E8%AF%AD/%E6%83%B3%E8%B1%A1%E5%AE%9E%E4%BD%93%E5%9B%BE%E9%89%B4&#x27;]    def parse(self, response):        ###### 下面是自己对html数据的处理逻辑        # index      //*[@id=&quot;mw-content-text&quot;]/div[1]        # number     //*[@id=&quot;mw-content-text&quot;]/div[1]/table[2]/tbody/tr[1]/th[1]        #            //*[@id=&quot;mw-content-text&quot;]/div[1]/table[2]/tbody/tr[1]/th[1]         # name       //*[@id=&quot;mw-content-text&quot;]/div[1]/table[2]/tbody/tr[1]/th[2]        # cost       //*[@id=&quot;mw-content-text&quot;]/div[1]/table[2]/tbody/tr[3]/td[1]/div/span        # feature    //*[@id=&quot;mw-content-text&quot;]/div[1]/table[2]/tbody/tr[3]/td[2]/b        # story      //*[@id=&quot;mw-content-text&quot;]/div[1]/table[2]/tbody/tr[3]/td[2]/i        movie_list = response.xpath(&quot;//*[@id=\\&quot;mw-content-text\\&quot;]/div[1]/table&quot;)        if movie_list and len(movie_list) &gt; 0:            for movie in movie_list:                item = DoubanItem()                item[&#x27;numb&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[1]/th[1]/text())&quot;).get()                item[&#x27;name&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[1]/th[2]/text())&quot;).extract()[0]                item[&#x27;cost&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[3]/td[1]/div/span/text())&quot;).extract()[0]                if item[&#x27;cost&#x27;]:                    item[&#x27;feature&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[3]/td[2]/b)&quot;).get()                    item[&#x27;story&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[3]/td[2]/i/text())&quot;).extract()[0]                else:                    item[&#x27;feature&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[3]/td/b)&quot;).get()                    item[&#x27;story&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[3]/td/i/text())&quot;).extract()[0]                    item[&#x27;cost&#x27;] = movie.xpath(&quot;normalize-space(./tbody/tr[4]/td/span)&quot;).get()                # item[&#x27;obtain&#x27;] = [movie.xpath(f&quot;normalize-space(./div[7]/div[&#123;i&#125;]/text())&quot;).get()                #                   for i in range(1,5)]                # item[&#x27;tag&#x27;] = movie.xpath(&quot;normalize-space(./div[8]/div[1]/text())&quot;).get(),movie.xpath(&quot;normalize-space(./div[8]/div[2]/text())&quot;).extract()[0],movie.xpath(&quot;normalize-space(./div[8]/div[3]/text())&quot;).extract()[0],movie.xpath(&quot;normalize-space(./div[8]/div[4]/text())&quot;).extract()[0]                # item[&#x27;feature&#x27;] = movie.xpath(&quot;normalize-space(./div[9]/div/div)&quot;).get()                yield item\nitems文件用来接收爬取的信息。只保留需要的，不需要的我一般注释掉。\n# Define here the models for your scraped items## See documentation in:# https://docs.scrapy.org/en/latest/topics/items.htmlimport scrapyclass DoubanItem(scrapy.Item):    # define the fields for your item here like:    # name = scrapy.Field()    # cname = scrapy.Field()    # ename = scrapy.Field()    # jname = scrapy.Field()    # code = scrapy.Field()    # sub_occupation = scrapy.Field()    # influnce = scrapy.Field()    # place_of_birth = scrapy.Field()    # race = scrapy.Field()    # hp = scrapy.Field()    # atk = scrapy.Field()    # defe = scrapy.Field()    # res = scrapy.Field()    # re_deploy = scrapy.Field()    # cost = scrapy.Field()    # block = scrapy.Field()    # interval = scrapy.Field()    # sex = scrapy.Field()    # position = scrapy.Field()    # obtain = scrapy.Field()    # tag = scrapy.Field()    # feature=scrapy.Field()# # 干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取干员爬取    # numb =scrapy.Field()    # name =scrapy.Field()    # cost =scrapy.Field()    # feature =scrapy.Field()    # story =scrapy.Field()    # #藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取藏品爬取    pass\n数据整理将json数据转换进excel里\n干员整理import pandas as pdimport jsonfrom pathlib import Pathjson_files = [&quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/先锋.json&quot;,              &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/近卫男.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/近卫女.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/狙击.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/术师.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/医疗.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/重装.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/辅助.json&quot;,               &quot;C:/Users/86183/Documents/WeChat Files/wxid_ot8jppgzo67v22/FileStorage/File/2024-07/特种.json&quot;]all_data=[]for file in json_files:    with open(file, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:        data = json.load(f)        all_data.extend(data)  # 将当前 JSON 文件的数据扩展到 all_data 列表中df = pd.DataFrame(all_data)data=df.rename(columns=&#123;    &#x27;cname&#x27;:&#x27;中文名&#x27;,    &#x27;ename&#x27;:&#x27;英文名&#x27;,    &#x27;jname&#x27;:&#x27;日文名&#x27;,    &#x27;code&#x27;:&#x27;编号&#x27;,    &#x27;sub_occupation&#x27;:&#x27;子职业&#x27;,    &#x27;influnce&#x27;:&#x27;归属势力&#x27;,    &#x27;place_of_birth&#x27;:&#x27;出生地&#x27;,    &#x27;race&#x27;:&#x27;种族&#x27;,    &#x27;hp&#x27;:&#x27;血量&#x27;,    &#x27;atk&#x27;:&#x27;攻击&#x27;,    &#x27;defe&#x27;:&#x27;物防&#x27;,    &#x27;res&#x27;:&#x27;法防&#x27;,    &#x27;re_deploy&#x27;:&#x27;再部署时间&#x27;,    &#x27;cost&#x27;:&#x27;费用&#x27;,    &#x27;block&#x27;:&#x27;阻挡数&#x27;,    &#x27;interval&#x27;:&#x27;攻速&#x27;,    &#x27;sex&#x27;:&#x27;性别&#x27;,    &#x27;position&#x27;:&#x27;定位&#x27;,    &#x27;obtain&#x27;:&#x27;获取方式&#x27;,    &#x27;tag&#x27;:&#x27;标签&#x27;,    &#x27;feature&#x27;:&#x27;特性&#x27;    &#125;)data.to_excel(&#x27;干员总览.xlsx&#x27;, index=False)  # 如果不需要保存索引列，可以设置index参数为False\n藏品整理import pandas as pdimport jsonfrom pathlib import Pathjson_files = [&quot;C:/Users/86180/OneDrive/桌面/python/douban/quotes.json&quot;]all_data=[]for file in json_files:    with open(file, &#x27;r&#x27;, encoding=&#x27;utf-8&#x27;) as f:        data = json.load(f)        all_data.extend(data)  # 将当前 JSON 文件的数据扩展到 all_data 列表中df = pd.DataFrame(all_data)data=df.rename(columns=&#123;    &#x27;numb&#x27;:&#x27;编号&#x27;,    &#x27;name&#x27;:&#x27;名字&#x27;,    &#x27;cost&#x27;:&#x27;获取&#x27;,    &#x27;feature&#x27;:&#x27;效果&#x27;,    &#x27;story&#x27;:&#x27;故事&#x27;    &#125;)data.to_excel(&#x27;萨卡兹的无终奇语想象实体图鉴.xlsx&#x27;, index=False)  # 如果不需要保存索引列，可以设置index参数为False\n数据分析饼状图df=pd.read_excel(&quot;C:/Users/86183/Desktop/总览/json to csv or excel/干员总览.xlsx&quot;,sheet_name=&#x27;Sheet1&#x27;)# 查看读取的数据df.head()# 打印前几行数据\npower_count = df[&#x27;所属势力&#x27;].value_counts().reset_index()#df中所属势力下的所有值数个数排成dataframe，重新设置index# print(power_count)temp = 10  # 设定频数界限than_ten = power_count[power_count[&#x27;count&#x27;] &gt;= temp].copy()lower_than_ten_sum = power_count[power_count[&#x27;count&#x27;] &lt; temp][&#x27;count&#x27;].sum()# 将小于频数界限的数据合并到一个 &#x27;其他&#x27; 类别other_row = pd.DataFrame([&#123;&#x27;所属势力&#x27;: &#x27;其他&#x27;, &#x27;count&#x27;: lower_than_ten_sum&#125;])result=pd.concat([than_ten,other_row])# 可选的饼图配置explode = [0.02] * len(result)  # 各个分块的间距colors = [&#x27;lavender&#x27;, &#x27;g&#x27;, &#x27;r&#x27;, &#x27;c&#x27;, &#x27;m&#x27;, &#x27;y&#x27;, &#x27;cyan&#x27;, &#x27;pink&#x27;, &#x27;orange&#x27;, &#x27;grey&#x27;][:len(than_ten)]# 绘制饼图plt.figure(figsize=(20, 20))plt.pie(result[&#x27;count&#x27;], explode=explode, labels=result[&#x27;所属势力&#x27;],        colors=colors, autopct=&#x27;%2.1f%%&#x27;, textprops=&#123;&#x27;fontsize&#x27;: 24&#125;)plt.title(&#x27;干员所属势力分布饼图&#x27;)plt.savefig(&#x27;干员所属势力分布饼图.png&#x27;)plt.show()\n直方图# 读取数据df=pd.read_excel(&quot;C:/Users/86180/OneDrive/桌面/python/干员总览.xlsx&quot;,sheet_name=&#x27;Sheet1&#x27;)# 定义一个生成直方图的方法，参数有：‘标题’，标题大小，‘x轴名’，‘y轴名’，x轴旋转，x轴大小，y轴大小def plot_bar_set(title,title_fontsize, xlabel, ylabel, x_rotation=0, x_fontsize=14, y_fontsize=14):    plt.title(title,fontsize=title_fontsize)    plt.xlabel(xlabel)    plt.ylabel(ylabel)    plt.xticks(rotation=x_rotation, fontsize=x_fontsize)    plt.yticks(fontsize=y_fontsize)# 查看读取的数据df.head()# 打印前几行数据\n# 使用df[&#x27;子职业&#x27;].value_counts()统计了DataFrame df中&#x27;子职业&#x27;列的每个唯一值的出现次数，并将结果存储在grouped变量中。grouped = df[&#x27;子职业&#x27;].value_counts().reset_index()# 绘制了柱状图，x轴为&#x27;子职业&#x27;，y轴为&#x27;count&#x27;，数据源为groupedplt.figure(figsize=(120,40))# 绘制了柱状图，x轴为&#x27;子职业&#x27;，y轴为&#x27;count&#x27;，数据源为groupedplt.bar(&#x27;子职业&#x27;,&#x27;count&#x27;,data=grouped,width=0.7)plot_bar_set(&#x27;干员子职业分布统计&#x27;,150,&#x27;子职业&#x27;,&#x27;个数&#x27;,45,64,64)# 保存plt.savefig(&#x27;干员子职业总览.png&#x27;)\n散点图#  提取攻击和血量数据attack = df[&#x27;攻击&#x27;]hp = df[&#x27;血量&#x27;]#  绘制散点图plt.figure(figsize=(10, 6))  # 设置图的大小plt.scatter(hp,attack, marker=&#x27;o&#x27;, color=&#x27;blue&#x27;, alpha=0.5)  # 绘制散点图plt.title(&#x27;干员攻击与血量散点图&#x27;)  # 设置图的标题plt.xlabel(&#x27;血量&#x27;)  # 设置 x 轴标签plt.ylabel(&#x27;攻击&#x27;)  # 设置 y 轴标签plt.grid(True)  # 添加网格线plt.tight_layout()  # 调整布局，防止标签被切割plt.savefig(&#x27;干员攻击与血量散点图.png&#x27;)plt.show()  # 显示图形\n子职业相关这里比较多我们只展示其中之一————DPS\nimport pandas as pdimport openpyxlimport matplotlib.pyplot as plt# 设置中文及字符显示plt.rcParams[&#x27;font.sans-serif&#x27;] = &#x27;SimSun&#x27;plt.rcParams[&#x27;axes.unicode_minus&#x27;] = Falsedf = pd.read_excel(&#x27;干员总览.xlsx&#x27;, sheet_name=&#x27;Sheet1&#x27;)df.head()df[&#x27;攻速&#x27;] = df[&#x27;攻速&#x27;].str.replace(&#x27;s&#x27;, &#x27;&#x27;).astype(float)df[&#x27;普攻DPS&#x27;] = df[&#x27;攻击&#x27;] / df[&#x27;攻速&#x27;]df_sorted = df.sort_values(by=&#x27;子职业&#x27;)average_dps = (df_sorted.groupby(&#x27;子职业&#x27;)[&#x27;普攻DPS&#x27;].mean().reset_index())average_dps = average_dps.sort_values(by=&#x27;普攻DPS&#x27;)output_file = &#x27;干员普攻DPS统计.xlsx&#x27;with pd.ExcelWriter(output_file, engine=&#x27;openpyxl&#x27;) as writer:    average_dps.to_excel(writer, sheet_name=&#x27;普攻DPS统计&#x27;, index=False)plt.figure(figsize=(80, 40))plt.bar(average_dps[&#x27;子职业&#x27;], average_dps[&#x27;普攻DPS&#x27;], color=&#x27;skyblue&#x27;)plt.xticks(fontsize=60)plt.yticks(fontsize=60)plt.xlabel(&#x27;子职业&#x27;,fontsize=60)plt.ylabel(&#x27;平均普攻DPS&#x27;,fontsize=60)plt.title(&#x27;不同子职业的平均普攻DPS&#x27;,fontsize=60)plt.xticks(rotation=45)plt.tight_layout()plt.savefig(&#x27;平均普攻DPS柱状图.png&#x27;)plt.show()\n部分成果\n改进我们发现Wiki角色的详细界面的格式均为https://prts.wiki/w/nymph我们可以通过爬取的信息重写一个爬虫，循环爬取所有的角色详情页面。\n","categories":["课外学习"],"tags":["python","明日方舟","爬虫","游戏"]},{"title":"Hello World","url":"/2024/07/24/hello-world/","content":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.\nQuick StartCreate a new post$ hexo new &quot;My New Post&quot;\n\nMore info: Writing\nRun server$ hexo server\n\nMore info: Server\nGenerate static files$ hexo generate\n\nMore info: Generating\nDeploy to remote sites$ hexo deploy\n\nMore info: Deployment\n"},{"title":"音响/音乐小夜灯","url":"/2024/07/28/%E9%9F%B3%E5%93%8D-%E9%9F%B3%E4%B9%90%E5%B0%8F%E5%A4%9C%E7%81%AF/","content":"前言这是我们在大一学业空闲时间的小作品，一个印象+一个小夜灯，目前都已投入使用^ ^参与这个作品的两位嗣凌sama和the RUN我们参考了zutterhao的开源代码https://www.cnblogs.com/zutterhao/p/10140172.html以下是我们的库音乐小夜灯你也可以通过在终端运行以下代码下载\ngit clone git@gitee.com:Siling402/light.gitgit clone https://gitee.com/Siling402/light.git ##任选一行执行\n\n由于我们是第一次制作花费了很多耗材 &gt; &lt; 如果你要复刻记得多买点材料！！\n耗材奥松机器人声音传感器\ncc2541蓝牙模块板4.0\nArduino UNO\nWS2812B\n等，详细材料前面的两个参考链接里有。\n音响展示视频：音响视频\n\n\n小夜灯展示视频：小夜灯视频\n\n\n\n","categories":["手工"],"tags":["arduino","手工","课程设计"]},{"title":"小刻横板卷轴格斗游戏","url":"/2024/07/28/%E5%B0%8F%E5%88%BB%E6%A8%AA%E6%9D%BF%E5%8D%B7%E8%BD%B4%E6%A0%BC%E6%96%97%E6%B8%B8%E6%88%8F/","content":"\n\n前言这是我大一课余自学UE5制作的一个简陋的小游戏，代码由我完成，嗣凌sama和the RUN帮忙绘制了大部分敌人的动作\n如果你想体验游戏，你可以从这个链接获取\n链接：百度网盘提取码：0eqd\n我们的所有自制素材也可以通过以下链接获取！\n链接：百度网盘提取码：s3bf\n项目的github地址：Ceobe2d\n感谢支持！\n^ ^\n预览大一课余自制的明日方舟小游戏\n ","categories":["游戏项目"],"tags":["明日方舟","游戏","课程设计","Unreal Engine","同人游戏"]},{"title":"game","url":"/game/index.html","content":""},{"title":"archives","url":"/archives/index.html","content":""},{"title":"archives","url":"/archives/index-1.html","content":""}]